# [0xx][sparse pattern][attention and FFN] Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design
## Overview
* Authors:
* Affiliations: 
* Publication Venue: 
* Link: []()
## Summary: 
### Problem:
- length of input could affect the computation overhead proportion of each layer, hence only target on FFN or only on attention will loss the opportunity to achieve the optimal design.
- Design goal:
  1. A pattern has strucutred data access
  2. Could caputure both local info and global info
  3. Could be applied both on FFN and attention.

### Key idea: 
- 
### Takeaways: 
### Strengths: 
### weaknesses: 
### How can you do better:
### Comments
