# [0xx][vision transformer acceleration] ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design
## Overview
* Authors: Haoran You, Zhanyi Sun
* Affiliations: 
* Publication Venue: 
* Link: []()
## Summary: 
### Problem:
1. ViTs have fixed number of input tokens during both training and inference, hence there are oppotunities to avoid on-the-fly sparse attention pattern prediction.
2. ViT allows high sparse ratio which aggravate the irregular data access and processing, which lead to utilization problem.
### Key idea: 
### Takeaways: 
### Strengths: 
### weaknesses: 
### How can you do better:
### Comments